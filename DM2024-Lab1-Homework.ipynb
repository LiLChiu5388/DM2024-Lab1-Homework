{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Information:\n",
    "Name: 邱子恩\n",
    "\n",
    "Student ID: 113065526\n",
    "\n",
    "GitHub ID: LiLChiu5388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Arthur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Arthur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Arthur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Install huggingface_hub to access HfFileSystem",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\registry.py:242\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[1;34m(protocol)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     register_implementation(protocol, _import_class(bit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\registry.py:277\u001b[0m, in \u001b[0;36m_import_class\u001b[1;34m(fqp)\u001b[0m\n\u001b[0;32m    276\u001b[0m is_s3 \u001b[38;5;241m=\u001b[39m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3fs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 277\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_s3 \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, vectorizer\n\u001b[0;32m     66\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 67\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m prepare_dataset(data_path)\n\u001b[0;32m     68\u001b[0m X, vectorizer \u001b[38;5;241m=\u001b[39m create_features(processed_df)\n\u001b[0;32m     69\u001b[0m y \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(data_path):\n\u001b[1;32m---> 17\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# 1. Drop duplicates\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\pandas\\io\\common.py:430\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m    431\u001b[0m         filepath_or_buffer, mode\u001b[38;5;241m=\u001b[39mfsspec_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m    432\u001b[0m     )\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\core.py:484\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(urlpath, mode, compression, encoding, errors, protocol, newline, expand, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a path or paths, return one ``OpenFile`` object.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m  https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m expand \u001b[38;5;241m=\u001b[39m DEFAULT_EXPAND \u001b[38;5;28;01mif\u001b[39;00m expand \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m expand\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m open_files(\n\u001b[0;32m    485\u001b[0m     urlpath\u001b[38;5;241m=\u001b[39m[urlpath],\n\u001b[0;32m    486\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    487\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    488\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    489\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    490\u001b[0m     protocol\u001b[38;5;241m=\u001b[39mprotocol,\n\u001b[0;32m    491\u001b[0m     newline\u001b[38;5;241m=\u001b[39mnewline,\n\u001b[0;32m    492\u001b[0m     expand\u001b[38;5;241m=\u001b[39mexpand,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(urlpath)\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\core.py:295\u001b[0m, in \u001b[0;36mopen_files\u001b[1;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_files\u001b[39m(\n\u001b[0;32m    217\u001b[0m     urlpath,\n\u001b[0;32m    218\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    229\u001b[0m ):\n\u001b[0;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given a path or paths, return a list of ``OpenFile`` objects.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    For writing, a str path must contain the \"*\" character, which will be filled\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     fs, fs_token, paths \u001b[38;5;241m=\u001b[39m get_fs_token_paths(\n\u001b[0;32m    296\u001b[0m         urlpath,\n\u001b[0;32m    297\u001b[0m         mode,\n\u001b[0;32m    298\u001b[0m         num\u001b[38;5;241m=\u001b[39mnum,\n\u001b[0;32m    299\u001b[0m         name_function\u001b[38;5;241m=\u001b[39mname_function,\n\u001b[0;32m    300\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m    301\u001b[0m         protocol\u001b[38;5;241m=\u001b[39mprotocol,\n\u001b[0;32m    302\u001b[0m         expand\u001b[38;5;241m=\u001b[39mexpand,\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mprotocol \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    305\u001b[0m         fs\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;241m=\u001b[39m auto_mkdir\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\core.py:648\u001b[0m, in \u001b[0;36mget_fs_token_paths\u001b[1;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol:\n\u001b[0;32m    647\u001b[0m     storage_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m protocol\n\u001b[1;32m--> 648\u001b[0m chain \u001b[38;5;241m=\u001b[39m _un_chain(urlpath0, storage_options \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m    649\u001b[0m inkwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# Reverse iterate the chain, creating a nested target_* structure\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\core.py:344\u001b[0m, in \u001b[0;36m_un_chain\u001b[1;34m(path, kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(bits):\n\u001b[0;32m    343\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m split_protocol(bit)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m get_filesystem_class(protocol)\n\u001b[0;32m    345\u001b[0m     extra_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_kwargs_from_urls(bit)\n\u001b[0;32m    346\u001b[0m     kws \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(protocol, {})\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\Lib\\site-packages\\fsspec\\registry.py:244\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[1;34m(protocol)\u001b[0m\n\u001b[0;32m    242\u001b[0m         register_implementation(protocol, _import_class(bit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(bit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merr\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m registry[protocol]\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mImportError\u001b[0m: Install huggingface_hub to access HfFileSystem"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import math\n",
    "import PAMI\n",
    "import umap\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def prepare_dataset(data_path):\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # 1. Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    df = df.dropna(subset=['sentiment', 'comment'])\n",
    "    \n",
    "    # 3. Text preprocessing function\n",
    "    def preprocess_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    df['processed_comment'] = df['comment'].apply(preprocess_text)\n",
    "    \n",
    "    # 4. Convert sentiment to numerical values for nostalgia classification\n",
    "    sentiment_map = {'nostalgia': 1, 'not nostalgia': 0}  \n",
    "    df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
    "    \n",
    "    print(\"Unique sentiments in original data:\", df['sentiment'].unique())\n",
    "    print(\"Unique sentiment labels after mapping:\", df['sentiment_label'].unique())\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# create feature vectors\n",
    "def create_features(df, max_features=5000):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(df['processed_comment'])\n",
    "    \n",
    "    print(f\"\\nNumber of features created: {X.shape[1]}\")\n",
    "    print(\"Sample feature names:\", list(vectorizer.get_feature_names_out())[:10])\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "data_path = \"hf://datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv\"\n",
    "processed_df = prepare_dataset(data_path)\n",
    "X, vectorizer = create_features(processed_df)\n",
    "y = processed_df['sentiment_label'].values\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total number of samples: {len(processed_df)}\")\n",
    "print(f\"Number of nostalgia comments: {sum(y == 1)}\")\n",
    "print(f\"Number of non-nostalgia comments: {sum(y == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_comments(comments, sentiments):\n",
    "    \"\"\"Format comments and sentiments into a list of dictionaries\"\"\"\n",
    "    return [{'comment': comment, 'sentiment': sentiment} \n",
    "            for comment, sentiment in zip(comments, sentiments)]\n",
    "\n",
    "# 令processed_df是之前處理好的數據\n",
    "data = pd.DataFrame.from_records(format_comments(processed_df['processed_comment'], \n",
    "                                               processed_df['sentiment']))\n",
    "\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "\n",
    "print(\"\\nFirst two comments:\")\n",
    "for comment in data[\"comment\"][:2]:\n",
    "    print(comment)\n",
    "\n",
    "data['sentiment_value'] = processed_df['sentiment_label']\n",
    "\n",
    "data['comment_length'] = data['comment'].str.len()\n",
    "data['sentiment_intensity'] = data['comment_length'].apply(lambda x: \n",
    "    'high' if x > 100 else 'medium' if x > 50 else 'low')\n",
    "\n",
    "# 添加時間戳列（如果原始數據中有的話）\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    data['timestamp'] = processed_df['timestamp']\n",
    "\n",
    "print(\"\\nFirst 10 records (comment and sentiment):\")\n",
    "print(data.loc[:10, ['comment', 'sentiment']])\n",
    "\n",
    "print(\"\\nUsing iloc for the first 10 comments:\")\n",
    "print(data.iloc[:10, 0])\n",
    "\n",
    "print(\"\\nLast 10 records:\")\n",
    "print(data.iloc[-10:])\n",
    "\n",
    "data['word_count'] = data['comment'].str.split().str.len()\n",
    "\n",
    "# 建立情感分布\n",
    "sentiment_summary = data.groupby('sentiment').agg({\n",
    "    'comment': 'count',\n",
    "    'word_count': ['mean', 'median', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nSentiment Distribution Summary:\")\n",
    "print(sentiment_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_sample, _ \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      9\u001b[0m sns\u001b[38;5;241m.\u001b[39mcountplot(data\u001b[38;5;241m=\u001b[39mX_sample, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "X_sample, _ = train_test_split(data, test_size=0.8, stratify=data['sentiment'], random_state=42)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=X_sample, x='sentiment')\n",
    "plt.title('Sentiment Distribution in Sample')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=X_sample, x='sentiment', y='word_count')\n",
    "plt.title('Word Count Distribution by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from scipy.stats import skew\n",
    "\n",
    "def create_text_features(df):\n",
    "    # Text-based features\n",
    "    df['caps_ratio'] = df['comment'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "    df['has_question'] = df['comment'].apply(lambda x: '?' in x).astype(int)\n",
    "    df['has_exclamation'] = df['comment'].apply(lambda x: '!' in x).astype(int)\n",
    "    \n",
    "    # Sentiment intensity features\n",
    "    df['polarity'] = df['comment'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    \n",
    "    # Statistical features\n",
    "    df['word_avg_length'] = df['comment'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "    df['word_skew'] = df['comment'].apply(lambda x: skew([len(word) for word in x.split()]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = create_text_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature Subset Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feature_cols = ['word_count', 'caps_ratio', 'polarity', 'subjectivity', 'word_avg_length']\n",
    "X_features = data[feature_cols]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Select top k features\n",
    "k = 3\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_selected = selector.fit_transform(X_scaled, data['sentiment_value'])\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = np.array(feature_cols)[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Attribute Transformation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "def transform_attributes(df, features):\n",
    "   \n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), \n",
    "                           columns=[f\"{col}_scaled\" for col in features])\n",
    "    \n",
    "    # Power transformation\n",
    "    pt = PowerTransformer()\n",
    "    df_power = pd.DataFrame(pt.fit_transform(df[features]), \n",
    "                          columns=[f\"{col}_power\" for col in features])\n",
    "    \n",
    "    # Log transformation for positive features\n",
    "    df_log = pd.DataFrame()\n",
    "    for col in features:\n",
    "        if (df[col] > 0).all():\n",
    "            df_log[f\"{col}_log\"] = np.log1p(df[col])\n",
    "    \n",
    "    return pd.concat([df_scaled, df_power, df_log], axis=1)\n",
    "\n",
    "transformed_features = transform_attributes(data, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "numeric_features = data[selected_features]\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(numeric_features)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(numeric_features)\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot PCA\n",
    "scatter1 = ax1.scatter(pca_result[:, 0], pca_result[:, 1], \n",
    "                      c=data['sentiment_value'], cmap='viridis')\n",
    "ax1.set_title('PCA Results')\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Plot t-SNE\n",
    "scatter2 = ax2.scatter(tsne_result[:, 0], tsne_result[:, 1], \n",
    "                      c=data['sentiment_value'], cmap='viridis')\n",
    "ax2.set_title('t-SNE Results')\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratio for PCA\n",
    "print(\"PCA explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer, Binarizer\n",
    "\n",
    "# Discretization\n",
    "kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "discretized_features = pd.DataFrame(kbd.fit_transform(data[selected_features]), \n",
    "                                  columns=[f\"{col}_disc\" for col in selected_features])\n",
    "\n",
    "# Binarization\n",
    "binarizer = Binarizer()\n",
    "binary_features = pd.DataFrame(binarizer.fit_transform(data[selected_features]), \n",
    "                             columns=[f\"{col}_bin\" for col in selected_features])\n",
    "\n",
    "data = pd.concat([data, discretized_features, binary_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Document Similarity Analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# Select three sample\n",
    "sample_docs = df['processed_comment'].head(3).tolist()\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(sample_docs)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            cmap='YlOrRd', \n",
    "            xticklabels=['Doc 1', 'Doc 2', 'Doc 3'],\n",
    "            yticklabels=['Doc 1', 'Doc 2', 'Doc 3'])\n",
    "plt.title('Document Similarity Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print the original documents for reference\n",
    "print(\"\\nOriginal documents used for comparison:\")\n",
    "for i, doc in enumerate(df['comment'].head(3), 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(doc)\n",
    "    print(f\"Processed version:\")\n",
    "    print(df['processed_comment'].iloc[i-1])\n",
    "\n",
    "# Additional similarity analysis\n",
    "print(\"\\nPairwise similarity scores:\")\n",
    "for i in range(len(sample_docs)):\n",
    "    for j in range(i+1, len(sample_docs)):\n",
    "        print(f\"Similarity between Document {i+1} and Document {j+1}: {similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "# Analyze vocabulary overlap\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sample_docs)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nCommon terms across documents:\")\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Sample terms:\", sorted(vocabulary)[:10]) \n",
    "\n",
    "# Document statistics\n",
    "print(\"\\nDocument Statistics:\")\n",
    "for i, doc in enumerate(sample_docs, 1):\n",
    "    words = doc.split()\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Word count: {len(words)}\")\n",
    "    print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Requirement: TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference from internet, Scikit-learn, GAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,              # Min frequency\n",
    "    max_df=0.95,           # Max frequency\n",
    "    ngram_range=(1, 2)     # both unigrams and bigrams\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_df['processed_comment'])\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of TF-IDF features: {len(feature_names)}\")\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# 2. Analyze top terms by TF-IDF scores\n",
    "def get_top_tfidf_terms(tfidf_matrix, feature_names, n_top=10):\n",
    "    # Sum tfidf values\n",
    "    sums = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "    # Sort term-score pairs\n",
    "    terms_scores = [(term, score) for term, score in zip(feature_names, sums)]\n",
    "    terms_scores = sorted(terms_scores, key=lambda x: x[1], reverse=True)\n",
    "    return terms_scores[:n_top]\n",
    "\n",
    "top_terms = get_top_tfidf_terms(tfidf_matrix, feature_names)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "terms, scores = zip(*top_terms)\n",
    "plt.barh(terms, scores)\n",
    "plt.title('Top Terms by Overall TF-IDF Score')\n",
    "plt.xlabel('TF-IDF Score Sum')\n",
    "plt.ylabel('Terms')\n",
    "plt.show()\n",
    "\n",
    "# 3. Analyze TF-IDF scores by sentiment\n",
    "def get_sentiment_specific_tfidf(tfidf_matrix, sentiment_labels, feature_names, sentiment_value, n_top=10):\n",
    "    \n",
    "    sentiment_docs = tfidf_matrix[sentiment_labels == sentiment_value]\n",
    "    avg_scores = np.array(sentiment_docs.mean(axis=0)).flatten()\n",
    "    terms_scores = [(term, score) for term, score in zip(feature_names, avg_scores)]\n",
    "    return sorted(terms_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "\n",
    "nostalgia_terms = get_sentiment_specific_tfidf(tfidf_matrix, y, feature_names, 1)\n",
    "non_nostalgia_terms = get_sentiment_specific_tfidf(tfidf_matrix, y, feature_names, 0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# nostalgia\n",
    "terms, scores = zip(*nostalgia_terms)\n",
    "ax1.barh(terms, scores)\n",
    "ax1.set_title('Top Terms in Nostalgia Comments')\n",
    "ax1.set_xlabel('Average TF-IDF Score')\n",
    "\n",
    "# non-nostalgia\n",
    "terms, scores = zip(*non_nostalgia_terms)\n",
    "ax2.barh(terms, scores)\n",
    "ax2.set_title('Top Terms in Non-Nostalgia Comments')\n",
    "ax2.set_xlabel('Average TF-IDF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Create document similarity matrix using TF-IDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sample_size = min(1000, tfidf_matrix.shape[0])\n",
    "sample_indices = np.random.choice(tfidf_matrix.shape[0], sample_size, replace=False)\n",
    "sample_matrix = tfidf_matrix[sample_indices]\n",
    "similarity_matrix = cosine_similarity(sample_matrix)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix[:50, :50], cmap='YlOrRd')\n",
    "plt.title('Document Similarity Matrix (First 50 Documents)')\n",
    "plt.show()\n",
    "\n",
    "# 5. Analyze TF-IDF distribution\n",
    "sparsity = 1.0 - (np.count_nonzero(tfidf_matrix) / float(tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))\n",
    "print(f\"\\nTF-IDF matrix sparsity: {sparsity:.2%}\")\n",
    "\n",
    "tfidf_values = tfidf_matrix.data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tfidf_values, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of TF-IDF Values')\n",
    "plt.xlabel('TF-IDF Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tfidf_features = pd.DataFrame.sparse.from_spmatrix(\n",
    "    tfidf_matrix,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "print(\"\\nTF-IDF Statistics:\")\n",
    "print(f\"Average number of non-zero terms per document: {np.diff(tfidf_matrix.indptr).mean():.2f}\")\n",
    "print(f\"Maximum TF-IDF value: {tfidf_matrix.max():.4f}\")\n",
    "print(f\"Minimum non-zero TF-IDF value: {tfidf_matrix.data.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Requirement: Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#reference from internet, GAI\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Sentiment Distribution Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=processed_df, x='sentiment')\n",
    "plt.title('Distribution of Nostalgia vs Non-Nostalgia Comments')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2. Comment Length Analysis\n",
    "processed_df['comment_length'] = processed_df['comment'].str.len()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(data=processed_df, x='sentiment', y='comment_length')\n",
    "plt.title('Comment Length Distribution by Sentiment')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Comment Length')\n",
    "plt.show()\n",
    "\n",
    "# 3. Word Count Distribution\n",
    "processed_df['word_count'] = processed_df['processed_comment'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=processed_df, x='word_count', hue='sentiment', bins=50, multiple=\"layer\", alpha=0.6)\n",
    "plt.title('Word Count Distribution by Sentiment')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 4. Word Clouds for Nostalgia and Non-nos\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Nostalgia WordCloud\n",
    "plt.subplot(1, 2, 1)\n",
    "nostalgia_text = ' '.join(processed_df[processed_df['sentiment'] == 'nostalgia']['processed_comment'])\n",
    "wordcloud_nostalgia = WordCloud(width=800, height=400, background_color='white').generate(nostalgia_text)\n",
    "plt.imshow(wordcloud_nostalgia)\n",
    "plt.title('Most Common Words in Nostalgia Comments')\n",
    "plt.axis('off')\n",
    "\n",
    "# Non-nostalgia WordCloud\n",
    "plt.subplot(1, 2, 2)\n",
    "non_nostalgia_text = ' '.join(processed_df[processed_df['sentiment'] == 'not nostalgia']['processed_comment'])\n",
    "wordcloud_non_nostalgia = WordCloud(width=800, height=400, background_color='white').generate(non_nostalgia_text)\n",
    "plt.imshow(wordcloud_non_nostalgia)\n",
    "plt.title('Most Common Words in Non-Nostalgia Comments')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Top Words Comparison\n",
    "def get_top_words(text, n=20):\n",
    "    words = ' '.join(text).split()\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "nostalgia_words = get_top_words(processed_df[processed_df['sentiment'] == 'nostalgia']['processed_comment'])\n",
    "non_nostalgia_words = get_top_words(processed_df[processed_df['sentiment'] == 'not nostalgia']['processed_comment'])\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot for nostalgia\n",
    "plt.subplot(1, 2, 1)\n",
    "words, counts = zip(*nostalgia_words)\n",
    "plt.barh(words, counts)\n",
    "plt.title('Top 20 Words in Nostalgia Comments')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "# Plot for non-nostalgia\n",
    "plt.subplot(1, 2, 2)\n",
    "words, counts = zip(*non_nostalgia_words)\n",
    "plt.barh(words, counts)\n",
    "plt.title('Top 20 Words in Non-Nostalgia Comments')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Time Series Analysis (if timestamp is available)\n",
    "if 'timestamp' in processed_df.columns:\n",
    "    processed_df['date'] = pd.to_datetime(processed_df['timestamp'])\n",
    "    daily_sentiments = processed_df.groupby([processed_df['date'].dt.date, 'sentiment']).size().unstack()\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    daily_sentiments.plot(kind='line')\n",
    "    plt.title('Sentiment Trends Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Comments')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.show()\n",
    "\n",
    "# 7. Character Length Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=processed_df, x='sentiment', y='comment_length')\n",
    "plt.title('Comment Length Distribution (Violin Plot)')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Number of Characters')\n",
    "plt.show()\n",
    "\n",
    "# 8. Correlation Heatmap (if num features exist)\n",
    "numerical_features = processed_df.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_features) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(processed_df[numerical_features].corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Classificaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Requirement: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reference from internet, GAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_and_evaluate_nb_classifiers(X, text_column='text'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): DataFrame containing text data and categories\n",
    "    text_column (str): Name of the column containing text data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Results from both classifiers\n",
    "    \"\"\"\n",
    "    # Create category mapping\n",
    "    category_mapping = dict(X[['category', 'category_name']].drop_duplicates().values)\n",
    "    target_names = [category_mapping[label] for label in sorted(category_mapping.keys())]\n",
    "    \n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X[text_column], X['category'], test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 1. Word Frequency Based Classifier\n",
    "    print(\"Training Word Frequency Based Classifier...\")\n",
    "    count_vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train_raw)\n",
    "    X_test_counts = count_vectorizer.transform(X_test_raw)\n",
    "    \n",
    "    nb_freq_classifier = MultinomialNB()\n",
    "    nb_freq_classifier.fit(X_train_counts, y_train)\n",
    "    y_pred_freq = nb_freq_classifier.predict(X_test_counts)\n",
    "    \n",
    "    freq_accuracy = accuracy_score(y_test, y_pred_freq)\n",
    "    freq_report = classification_report(y_test, y_pred_freq, \n",
    "                                     target_names=target_names, \n",
    "                                     digits=4)\n",
    "    \n",
    "    # 2. TF-IDF Based Classifier\n",
    "    print(\"\\nTraining TF-IDF Based Classifier...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "    \n",
    "    # Convert sparse matrix to dense and scale features\n",
    "    X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "    X_test_tfidf_dense = X_test_tfidf.toarray()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf_dense)\n",
    "    X_test_tfidf_scaled = scaler.transform(X_test_tfidf_dense)\n",
    "    \n",
    "    # Use GaussianNB for TF-IDF features\n",
    "    nb_tfidf_classifier = GaussianNB()\n",
    "    nb_tfidf_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "    y_pred_tfidf = nb_tfidf_classifier.predict(X_test_tfidf_scaled)\n",
    "    \n",
    "    tfidf_accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "    tfidf_report = classification_report(y_test, y_pred_tfidf, \n",
    "                                       target_names=target_names, \n",
    "                                       digits=4)\n",
    "    \n",
    "    print(\"\\n=== Word Frequency Based Results ===\")\n",
    "    print(f\"Accuracy: {freq_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(freq_report)\n",
    "    \n",
    "    print(\"\\n=== TF-IDF Based Results ===\")\n",
    "    print(f\"Accuracy: {tfidf_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(tfidf_report)\n",
    "    \n",
    "    return {\n",
    "        'frequency_based': {\n",
    "            'vectorizer': count_vectorizer,\n",
    "            'classifier': nb_freq_classifier,\n",
    "            'accuracy': freq_accuracy,\n",
    "            'report': freq_report\n",
    "        },\n",
    "        'tfidf_based': {\n",
    "            'vectorizer': tfidf_vectorizer,\n",
    "            'classifier': nb_tfidf_classifier,\n",
    "            'accuracy': tfidf_accuracy,\n",
    "            'report': tfidf_report\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDifferences(v.s. MultinomialNB with TF-IDF (as MulNB)):\\n1. Feature: GauNB models distribution of TF-IDF values distribution more appropriately\\nbecause it treats them as continuous while MulNB interprets them as frequencies which they are not.\\n2. Task: GauNB is more suitable for capturing subtle differences in word usage frequencies within document\\nwhile MulNB is more suitable for handling discrete features in document classification tasks.\\n3. Performance: GauNB performs better when the distinctions between categories are based on word usage patterns\\nwhile MulNB performs better when there are clear keyword differences between categories.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I implemented GaussianNB with TF-IDF (as GauNB)\n",
    "\"\"\"\n",
    "Differences(v.s. MultinomialNB with TF-IDF (as MulNB)):\n",
    "1. Feature: GauNB models distribution of TF-IDF values distribution more appropriately\n",
    "because it treats them as continuous while MulNB interprets them as frequencies which they are not.\n",
    "2. Task: GauNB is more suitable for capturing subtle differences in word usage frequencies within document\n",
    "while MulNB is more suitable for handling discrete features in document classification tasks.\n",
    "3. Performance: GauNB performs better when the distinctions between categories are based on word usage patterns\n",
    "while MulNB performs better when there are clear keyword differences between categories.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Requirement: Thoughts of the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Current Limitations and Inefficiencies:\n",
    "\n",
    "1. Missing Values:\n",
    "\n",
    "    A. Current Approach: Directly removing missing values or filling them with a simple mean.\n",
    "\n",
    "    B. Issues:\n",
    "\n",
    "        1) Risk of losing valuable information, especially if missing data follows a specific pattern.\n",
    "        2) Using mean imputation may distort the true data distribution.\n",
    "        3) No consideration of the missing data mechanism (MCAR, MAR, MNAR).\n",
    "    \n",
    "    C. Possible improvements:\n",
    "\n",
    "        I. Recommendations:\n",
    "            1) Analyze missing patterns to understand the missing data mechanism.\n",
    "            2) Use advanced imputation techniques, such as:\n",
    "                MICE (Multiple Imputation by Chained Equations): Accounts for relationships between features.\n",
    "            3) Add an indicator for \"missingness\" as a new feature, which may have predictive value.\n",
    "        \n",
    "        II. Reasons:\n",
    "            1) Retains more original information.\n",
    "            2) Underlying data structure.\n",
    "        \n",
    "2. Feature Engineering:\n",
    "\n",
    "    A. Current Approach: Creating numerous frequency-based features.\n",
    "\n",
    "    B. Issues:\n",
    "\n",
    "        1) Overly sparse features, leading to higher computational cost.\n",
    "        2) Lack of consideration for the semantic relationships between words.\n",
    "        3) Potential for high correlation among features.\n",
    "\n",
    "    C. Possible improvements:\n",
    "\n",
    "        I. Recommendations:\n",
    "            1) Use advanced text features, including:\n",
    "                word embeddings (e.g., Word2Vec, BERT).\n",
    "\n",
    "            2) Incorporate domain knowledge, such as\n",
    "\n",
    "               specialized keyword dictionaries.\n",
    "        II. Reasons:\n",
    "            1) Captures deeper semantic information.\n",
    "            2) Increases feature interpretability.\n",
    "\n",
    "\n",
    "3. Dimensionality Reduction:\n",
    "\n",
    "    A. Current Approach: Direct application of PCA.\n",
    "\n",
    "    B. Issues:\n",
    "\n",
    "        1) Dimensionality reduction without prior feature importance assessment.\n",
    "        2) Risk of losing essential class-separating information.\n",
    "\n",
    "    C. Possible improvements:\n",
    "\n",
    "        I. Recommendations:\n",
    "            1) Perform feature selection, such as\n",
    "               Use Lasso or Ridge regression.\n",
    "            2) Employ nonlinear dimensionality reduction methods\n",
    "        II. Reasons:\n",
    "            1) Retains more classification information.\n",
    "\n",
    "4. Binarization:\n",
    "\n",
    "    A. Current Approach: Fixed threshold.\n",
    "\n",
    "    B. Issues:\n",
    "\n",
    "        1) Threshold selection lacks data-driven support.\n",
    "        2) No consideration of the distributional characteristics of features.\n",
    "        3) Potential information loss.\n",
    "\n",
    "    C. Possible improvements:\n",
    "\n",
    "        I. Recommendations:\n",
    "            1) Distribution-based thresholds\n",
    "            2) Supervised binarization, such as:\n",
    "                Maximize class separation.\n",
    "                Minimize information loss.\n",
    "        II. Reasons:\n",
    "            1) Adapts to data characteristics.\n",
    "            2) Retains essential information.\n",
    "            3) Enhances classification performance.\n",
    "\n",
    "5. Text Preprocessing:\n",
    "\n",
    "    A. Recommendations:\n",
    "        1) Include more NLP steps:\n",
    "            Lemmatization rather than stemming, etc.\n",
    "    B. Reason: Retains more semantic information, enhancing feature quality.\n",
    "\n",
    "6. Class Imbalance Handling:\n",
    "\n",
    "    A. Recommendations:\n",
    "        1) Use advanced oversampling methods like SMOTE or ADASYN.\n",
    "        2) Combine oversampling and undersampling.\n",
    "    B. Reason: Prevents model bias towards majority classes, improving minority class recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
